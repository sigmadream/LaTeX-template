\section{Intro}
준 지도학습(semi-supervised learning)이란 학습에서 목표값이 표시된(labeled) 데이터와 그렇지 않은(unlabeled) 데이터를 함께 사용하는 과정을 의미한다. 모든 데이터의 라벨이 표시된 지도학습(supervised learning)과 달리 학습 과정에서 여러 불확실성을 마주하게 된다. 하지만 수백만개의 모든 데이터에 일일이 태그를 다는데 걸리는 노동과 자원을 절약할 수 있으므로 더 경제적이고 범용적인 방법으로 볼 수 있다. \cite{zhu2006semisupervised}

Self-training은 준 지도학습을 수행하는 한 가지 방법이다. 우선 labeled 데이터만 가지고 모델을 학습하여 unlabeled 데이터에 대한 예측을 한다. 이를 기반으로 pseudo label을 만들어서 학습을 하고, 다시 예측으로 라벨을 생성하는 과정을 반복한다. 적은 수의 labeled 데이터만으로 학습한 모델은 오류율이 높으므로 생성된 pseudo label을 온전히 신뢰할 수 없다. 그러므로 일정 confidence threshold를 두고, 이를 넘는 것에 대해서만 학습 데이터에 추가하여 학습을 진행한다. 이는 가장 초기의 준 지도학습 구현 모델이며, 현재까지도 널리 쓰이는 방법 중 하나이다. \cite{zhu2006semisupervised}

ImageNet 분류기의 구현으로써, 당시 SOTA의 top-1 정확도 기록을 상회하여(88.4\%) 화제가 되었던 teacher-noisy student 모델에 대한 논문이 있었다. \cite{xie2020selftraining} Teacher와 student 두 모델을 두어, teacher가 생성한 pseudo label을 통해 student model이 학습을 하는 self-training 방식이었다. 특이한 점은 student model을 teacher보다 같거나 더 커다란 네트워크로 구축하여, 학습에 대한 잠재성을 높였다는데 있다. 또한 teacher 모델과 달리 student에만 강력한 노이즈를 두어, student는 teacher보다 더 어려운 방식으로 학습하도록 강제하였다. 저자는 이 방식이 teacher에서 student로 거듭 전수할수록 지식이 팽창되는 knowledge expansion 철학을 두었음을 강조하였으며, 이는 네트워크 경량화를 위해 사용된 knowledge distillation과는 \cite{hinton2015distilling} 반대 방향임을 언급했다.

Student model에 두 가지 종류의 noise를 제안하였다. 첫째, 데이터 증식에 사용한 input noise가 있다. 이는 이미지 분류에서 높은 효과를 보였던 RandAugment 방법이 \cite{cubuk2019randaugment} 대표적이다. 이것이 적용되지 않은 상태의 이미지를 분류하는 teacher은 깨끗한 pseudo label을 생성하고, student는 더 어려운 학습 과정을 거치게 된다. 둘째, model 자체에 들어간 noise이다. 이는 ResNet과 같은 skip connection을 지원하는 네트워크에 사용할 수 있는 stochastic depth와 \cite{huang2016deep}, 마지막 분류기에 넣기 위해 거치는 fully connected network에 추가하는 dropout이 있다. 이를 통해 학습 과정에서는 single model처럼 작동하지만, pseudo label을 생성하거나 최종 분류를 하는 추론 과정에서는 마치 ensemble처럼 작동이 가능했다고 저자는 설명했다. \cite{xie2020selftraining}

최근 이미지 분류기에 커다란 강직성을 주는 데이터 제공 방식으로 label smoothing \cite{szegedy2015rethinking} 이나 mixup \cite{zhang2018mixup} 등이 많이 적용되고 있다. 전자는 잘못된 label에 대한 가능성을 두고, 답으로 여겨지는 클래스가 아닌 것들에도 일부 점수를 주는 방식이다. 후자는 학습 데이터를 여러 이미지와 label을 혼합하여 구성하는 방법이다. 이를 통해 학습 데이터에 대한 더 generalized model을 습득할 수 있으며, FGSM과 같은 adversarial attack에 대해서 강직하게 된다고 한다. \cite{zhang2018mixup}

이번 프로젝트에서 나는 noisy student self-training의 소개 논문인 \cite{xie2020selftraining}를 CIFAR-10 데이터와 가벼운 네트워크로 구현해보았다. 그 과정에서 노이즈 종류와 데이터 제공 방식들이 적용되었을 때의 효과를 살펴보았다. 각 방식들이 모델에 어떤 향상을 가져오고, 어떤 tradeoff가 있었으며, 그 원인이 무엇인지를 탐구해보았다. 프로젝트 구현 코드와 구체적인 실행 방법은 다음 링크를 통해 접근할 수 있다. \\ \url{https://github.com/lego0901/embedded-noisystudent}